{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80aab074-534c-4fd0-9efa-453d61e1ecbe",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# **Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57868b6-00da-456b-8310-248743e28230",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (injected by pipeline)\n",
    "table_name = \"Customer\"  # default for local testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6d2ba2-cbb4-498c-b93b-89d1b703acd1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# **Imports & Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf0d43f-681f-49e1-ac28-29b09655ac9e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, col, trim, lower, initcap\n",
    "\n",
    "# Bronze OneLake path (get from Bronze lakehouse)\n",
    "bronze_base = \"abfss://[WORKSPACE_ID]@onelake.dfs.fabric.microsoft.com/[BRONZE_LAKEHOUSE_ID]/Files\"\n",
    "\n",
    "print(f\"üì• Processing table: {table_name}\")\n",
    "print(f\"üîó Bronze path: {bronze_base}/{table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929be49b-a80f-4b61-ad2e-70e31b9403df",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# **Read Bronze Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a544d3c3-f4c9-48e2-9024-040ee88bfedc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Read Parquet from Bronze\n",
    "bronze_path = f\"{bronze_base}/{table_name}\"\n",
    "\n",
    "try:\n",
    "    df = spark.read.parquet(bronze_path)\n",
    "    print(f\"‚úÖ Bronze data loaded: {df.count()} rows\")\n",
    "    df.show(5)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading Bronze: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc95613-1201-4b22-b329-db1fd5e69fac",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# **Apply Transformations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db77e7d8-2db6-4497-ad7a-da3a462a6167",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Basic cleaning - apply to all tables\n",
    "df_clean = df.withColumn(\"load_timestamp\", current_timestamp())\n",
    "\n",
    "# Table-specific transformations\n",
    "if table_name == \"Customer\":\n",
    "    df_clean = df_clean \\\n",
    "        .withColumn(\"FirstName\", initcap(trim(col(\"FirstName\")))) \\\n",
    "        .withColumn(\"LastName\", initcap(trim(col(\"LastName\")))) \\\n",
    "        .withColumn(\"EmailAddress\", lower(trim(col(\"EmailAddress\"))))\n",
    "    print(\"‚úÖ Names standardized (proper case), emails lowercase\")\n",
    "\n",
    "elif table_name == \"Product\":\n",
    "    # Future: Add product-specific transformations\n",
    "    pass\n",
    "\n",
    "elif table_name == \"SalesOrderHeader\":\n",
    "    # Future: Add date validation\n",
    "    pass\n",
    "\n",
    "# Deduplication (based on first column as business key)\n",
    "business_key = df_clean.columns[0]\n",
    "df_clean = df_clean.dropDuplicates([business_key])\n",
    "\n",
    "print(f\"‚úÖ Transformations applied: {df_clean.count()} rows after cleaning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d7b9c-6a01-4a87-93dc-8fa3e2c48746",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# **Write to Silver (Delta)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d91181-eb61-43ea-95e0-9a5779b2aa19",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Silver table name\n",
    "silver_table_name = f\"{table_name.lower()}_silver\"\n",
    "\n",
    "try:\n",
    "    # Write as Delta table\n",
    "    df_clean.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(silver_table_name)\n",
    "    \n",
    "    print(f\"‚úÖ Silver table created: {silver_table_name}\")\n",
    "    print(f\"üìä Final row count: {df_clean.count()}\")\n",
    "    \n",
    "    # Verify table exists\n",
    "    spark.sql(f\"DESCRIBE EXTENDED {silver_table_name}\").show(truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error writing to Silver: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "ecc4a5f6-de58-4003-8796-92c1572aefe8",
    "default_lakehouse_name": "silver_lakehouse",
    "default_lakehouse_workspace_id": "b355e309-abd3-46d6-a976-c5e6a37d6dd1",
    "known_lakehouses": [
     {
      "id": "ecc4a5f6-de58-4003-8796-92c1572aefe8"
     },
     {
      "id": "5984f7a1-e5b5-4746-b235-d681889aba3e"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
